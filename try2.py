#encoding=utf-8


import torch
import torch.nn as nn
import torch.autograd as autograd
from torch.autograd import Variable
embeddings = nn.ModuleList(
            [nn.Embedding(3309, 128)])
#embedding = nn.Embedding(3309, 128)
input = Variable(torch.LongTensor([[[67, 148, 39, 114, 115, 86, 73, 160, 281, 114, 115, 86, 73, 5, 717, 507, 797, 179, 372, 34, 39, 283, 704, 128, 23, 619, 704, 244, 588, 112, 275, 354, 2425, 2426, 13, 152, 153, 153, 153, 100, 110, 113, 717, 507, 45, 46, 39, 114, 115, 430, 63, 361, 326, 620, 1125, 1323, 23, 619, 44, 123, 112, 121, 576, 39, 114, 115, 588, 119, 149, 249, 23, 112, 46, 87, 444, 41, 441, 442, 39, 217, 704, 1935, 1143, 572, 215, 571, 572, 20, 34], [83, 249, 239, 744, 197, 558, 178, 854, 23, 93, 321, 191, 11, 11, 56, 57, 249, 523, 404, 306, 854, 289, 290, 23, 306, 854, 290, 13, 408, 802, 998, 508, 498, 23, 35, 22, 262, 217, 523, 404, 306, 309, 637, 357, 178, 854, 1168, 638, 545, 502, 854, 41, 443, 838, 770, 23, 86, 73, 178, 854, 578, 293, 306, 488, 112, 335, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]))
for emb,x in zip(embeddings,input):
	emb(x)
